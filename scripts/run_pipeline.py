#!/usr/bin/env python3
"""
YARA Pipeline Automation Script
Runs the complete pipeline: txt_to_json -> transpile_to_yara -> [optional] llm_validation
"""

import os
import sys
import subprocess
import argparse
import logging
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log')
    ]
)
logger = logging.getLogger(__name__)

class PipelineRunner:
    def __init__(self, data_dir="data", scripts_dir="scripts", input_files=None):
        self.data_dir = Path(data_dir)
        self.scripts_dir = Path(scripts_dir)
        self.workspace_root = Path(__file__).parent.parent
        
        # Ensure we're in the right directory
        os.chdir(self.workspace_root)
        
        # Define file paths
        if input_files:
            self.input_files = [Path(f) for f in input_files]
        else:
            self.input_files = [self.data_dir / "signature_patterns.txt"]
        
        # Default output files (will be overridden for multiple inputs)
        self.json_file = self.data_dir / "signatures.json"
        self.yara_file = self.data_dir / "yara_rules.yar"
        
        # Script paths
        self.txt_to_json_script = self.scripts_dir / "txt_to_json.py"
        self.transpile_script = self.scripts_dir / "transpile_to_yara.py"
        self.validation_script = self.scripts_dir / "llm_validation.py"
    
    def check_prerequisites(self):
        """Check if all required files and dependencies exist"""
        logger.info("üîç Checking prerequisites...")
        
        # Check if all input files exist
        for input_file in self.input_files:
            if not input_file.exists():
                raise FileNotFoundError(f"Input file not found: {input_file}")
        
        # Check if scripts exist
        if not self.txt_to_json_script.exists():
            raise FileNotFoundError(f"txt_to_json.py script not found: {self.txt_to_json_script}")
        
        if not self.transpile_script.exists():
            raise FileNotFoundError(f"transpile_to_yara.py script not found: {self.transpile_script}")
        
        # Check if data directory exists
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Data directory not found: {self.data_dir}")
        
        logger.info(f"‚úÖ All prerequisites met - {len(self.input_files)} input file(s) found")
    
    def run_txt_to_json(self):
        """Process all input files - convert text files to JSON, analyze binary files with LLM"""
        logger.info(f"üîÑ Step 1: Processing {len(self.input_files)} input file(s)...")
        
        text_results = []
        binary_results = []
        
        for i, input_file in enumerate(self.input_files):
            logger.info(f"üìÅ Processing input file {i+1}/{len(self.input_files)}: {input_file.name}")
            
            # Check file type
            if input_file.suffix.lower() == '.txt':
                # Text file - convert to JSON
                result = self._process_text_file(input_file, i)
                text_results.append(result)
            else:
                # Binary file - analyze with LLM
                result = self._process_binary_file(input_file, i)
                binary_results.append(result)
        
        # Process text files through JSON conversion
        if text_results:
            successful_text = [r for r in text_results if r['success']]
            if successful_text:
                logger.info(f"üîÑ Converting {len(successful_text)} text file(s) to JSON...")
                self.json_file = self._process_text_files_to_json(successful_text)
            else:
                logger.warning("‚ö†Ô∏è  No text files were successfully processed")
                self.json_file = self._create_empty_json_structure()
        else:
            logger.info("üìÅ No text files to process")
            self.json_file = self._create_empty_json_structure()
        
        # Store binary results for later YARA generation
        self.binary_yara_rules = []
        if binary_results:
            successful_binary = [r for r in binary_results if r['success']]
            if successful_binary:
                logger.info(f"üîÑ Processing {len(successful_binary)} binary file(s) with LLM...")
                self.binary_yara_rules = self._extract_binary_yara_rules(successful_binary)
                logger.info(f"‚úÖ Extracted {len(self.binary_yara_rules)} YARA rules from binary files")
            else:
                logger.warning("‚ö†Ô∏è  No binary files were successfully processed")
        
        logger.info("‚úÖ Step 1 completed")
    
    def _process_text_file(self, input_file: Path, index: int) -> Dict:
        """Process a single text file through txt_to_json.py"""
        try:
            # Create unique output filename for this input
            output_file = self.data_dir / f"signatures_{input_file.stem}_{index}.json"
            
            # Run txt_to_json.py
            cmd = [
                sys.executable,
                str(self.txt_to_json_script),
                str(input_file),
                "--output", str(output_file)
            ]
            
            logger.debug(f"üîß Running command: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=self.workspace_root,
                check=True,
                timeout=60
            )
            
            if result.returncode == 0:
                logger.info(f"‚úÖ Successfully converted {input_file.name} to JSON")
                return {
                    'success': True,
                    'input_file': input_file,
                    'output_file': output_file,
                    'index': index
                }
            else:
                logger.error(f"‚ùå txt_to_json.py failed for {input_file.name}")
                return {
                    'success': False,
                    'input_file': input_file,
                    'error': f"Exit code: {result.returncode}"
                }
                
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå txt_to_json.py failed for {input_file.name}: {e}")
            return {
                'success': False,
                'input_file': input_file,
                'error': str(e)
            }
        except Exception as e:
            logger.error(f"‚ùå Unexpected error processing {input_file.name}: {e}")
            return {
                'success': False,
                'input_file': input_file,
                'error': str(e)
            }
    
    def _process_binary_file(self, input_file: Path, index: int) -> Dict:
        """Process a single binary file through data_classifier.py with prompt optimization"""
        try:
            logger.info(f"üîç Analyzing binary file: {input_file.name}")
            
            # First, run prompt optimizer to analyze the file
            prompt_optimizer_cmd = [
                sys.executable,
                str(self.scripts_dir / "prompt_optimizer.py"),
                str(input_file)
            ]
            
            logger.debug(f"üîß Running prompt optimizer: {' '.join(prompt_optimizer_cmd)}")
            
            prompt_result = subprocess.run(
                prompt_optimizer_cmd,
                capture_output=True,
                text=True,
                cwd=self.workspace_root,
                check=True,
                timeout=60
            )
            
            if prompt_result.returncode == 0:
                logger.info(f"‚úÖ Prompt optimization completed for {input_file.name}")
                
                # Find the optimization results file
                optimization_files = list(Path('.').glob(f"prompt_optimization_{input_file.stem}_*.json"))
                if optimization_files:
                    # Use the most recent optimization file
                    latest_optimization = max(optimization_files, key=lambda f: f.stat().st_mtime)
                    
                    try:
                        with open(latest_optimization, 'r', encoding='utf-8') as f:
                            prompt_data = json.load(f)
                        optimized_prompt = prompt_data.get('optimized_prompt', '')
                        
                        if optimized_prompt:
                            logger.info(f"üìù Using optimized prompt for {input_file.name}")
                            logger.debug(f"üìã Prompt preview: {optimized_prompt[:200]}...")
                        else:
                            logger.warning(f"‚ö†Ô∏è  No optimized prompt found for {input_file.name}")
                            optimized_prompt = ""
                            
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è  Could not read optimization file for {input_file.name}: {e}")
                        optimized_prompt = ""
                else:
                    logger.warning(f"‚ö†Ô∏è  No optimization results file found for {input_file.name}")
                    optimized_prompt = ""
            else:
                logger.warning(f"‚ö†Ô∏è  Prompt optimization failed for {input_file.name}, using default")
                optimized_prompt = ""
            
            # Now run data classifier with the optimized prompt
            classifier_cmd = [
                sys.executable,
                str(self.scripts_dir / "data_classifier.py"),
                str(input_file)
            ]
            
            if optimized_prompt:
                classifier_cmd.extend(["--custom-prompt", optimized_prompt])
            
            logger.debug(f"üîß Running data classifier: {' '.join(classifier_cmd)}")
            
            result = subprocess.run(
                classifier_cmd,
                capture_output=True,
                text=True,
                cwd=self.workspace_root,
                check=True,
                timeout=120
            )
            
            if result.returncode == 0:
                logger.info(f"‚úÖ Data classification completed for {input_file.name}")
                
                # Save classification results
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                classification_file = self.data_dir / f"classification_{input_file.stem}_{timestamp}.json"
                
                # The data classifier saves results to a file, so we need to read that file
                # Find the classification results file
                classification_files = list(Path('.').glob(f"classification_{input_file.stem}_*.json"))
                if classification_files:
                    # Use the most recent classification file
                    latest_classification = max(classification_files, key=lambda f: f.stat().st_mtime)
                    
                    try:
                        with open(latest_classification, 'r', encoding='utf-8') as f:
                            classification_data = json.load(f)
                        
                        # Update the classification file path to the actual file found
                        classification_file = latest_classification
                        
                        logger.info(f"üíæ Classification results loaded from {classification_file}")
                        
                        return {
                            'success': True,
                            'input_file': input_file,
                            'classification_file': classification_file,
                            'classification_data': classification_data,
                            'index': index
                        }
                    except Exception as e:
                        logger.error(f"‚ùå Failed to read classification file for {input_file.name}: {e}")
                        return {
                            'success': False,
                            'input_file': input_file,
                            'error': f"File read error: {e}"
                        }
                else:
                    logger.warning(f"‚ö†Ô∏è  No classification results file found for {input_file.name}")
                    return {
                        'success': False,
                        'input_file': input_file,
                        'error': "No classification file found"
                    }
            else:
                logger.error(f"‚ùå Data classification failed for {input_file.name}")
                return {
                    'success': False,
                    'input_file': input_file,
                    'error': f"Exit code: {result.returncode}"
                }
                
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Binary file processing failed for {input_file.name}: {e}")
            return {
                'success': False,
                'input_file': input_file,
                'error': str(e)
            }
        except Exception as e:
            logger.error(f"‚ùå Unexpected error processing binary file {input_file.name}: {e}")
            return {
                'success': False,
                'input_file': input_file,
                'error': str(e)
            }
    
    def _process_text_files_to_json(self, text_results: List[Dict]) -> Path:
        """Combine multiple text file JSON outputs into a single file"""
        try:
            if len(text_results) == 1:
                # Single file, just rename it
                single_result = text_results[0]
                final_json = self.data_dir / "signatures.json"
                single_result['output_file'].rename(final_json)
                logger.info(f"üìÅ Single JSON file created: {final_json}")
                return final_json
            
            # Multiple files, combine them
            logger.info(f"üîÑ Combining {len(text_results)} JSON files...")
            
            combined_signatures = []
            combined_cleanup_constants = []
            
            for result in text_results:
                try:
                    with open(result['output_file'], 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # Collect signatures
                    if 'signatures' in data:
                        combined_signatures.extend(data['signatures'])
                    
                    # Collect cleanup constants
                    if 'cleanup_constants' in data:
                        combined_cleanup_constants.extend(data['cleanup_constants'])
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Could not read {result['output_file']}: {e}")
                    continue
            
            # Create combined JSON
            combined_data = {
                'cleanup_constants': combined_cleanup_constants,
                'signatures': combined_signatures
            }
            
            final_json = self.data_dir / "signatures_combined.json"
            with open(final_json, 'w', encoding='utf-8') as f:
                json.dump(combined_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üìÅ Combined JSON created: {final_json} ({len(combined_signatures)} signatures)")
            
            # Clean up individual files
            self._cleanup_individual_json_files(text_results)
            
            return final_json
            
        except Exception as e:
            logger.error(f"‚ùå Failed to combine JSON files: {e}")
            # Fallback to first successful result
            if text_results:
                return text_results[0]['output_file']
            else:
                raise
    
    def _cleanup_individual_json_files(self, text_results: List[Dict]):
        """Remove individual JSON files after combining"""
        for result in text_results:
            if 'output_file' in result and result['output_file'].exists():
                try:
                    result['output_file'].unlink()
                    logger.debug(f"üóëÔ∏è  Removed individual JSON: {result['output_file']}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Could not remove {result['output_file']}: {e}")
    
    def _extract_binary_yara_rules(self, binary_results: List[Dict]) -> List[str]:
        """Extract YARA rules from binary file classification results"""
        yara_rules = []
        processed_rules = set()
        
        for result in binary_results:
            if not result['success'] or 'classification_file' not in result:
                continue
            
            input_file = result['input_file']
            classification_file = result['classification_file']
            
            try:
                # Read classification results
                with open(classification_file, 'r', encoding='utf-8') as f:
                    classification_data = json.load(f)
                
                # Extract YARA rule from classification content
                classification_content = classification_data.get('classification_content', '')
                
                if not classification_content:
                    logger.warning(f"‚ö†Ô∏è  No classification content found for {input_file.name}")
                    continue
                
                # Look for YARA rule in the content
                # Try markdown format first (most common)
                rule_pattern = r'```yara\s*\n(.*?)\n```'
                rule_matches = re.findall(rule_pattern, classification_content, re.DOTALL | re.IGNORECASE)
                
                if not rule_matches:
                    # Try standard rule format without markdown
                    # Use brace counting to find complete rules
                    rule_start_pattern = r'rule\s+\w+\s*\{'
                    rule_starts = list(re.finditer(rule_start_pattern, classification_content, re.IGNORECASE))
                    
                    if rule_starts:
                        for match in rule_starts:
                            start_pos = match.start()
                            # Count braces to find the end
                            brace_count = 0
                            for i, char in enumerate(classification_content[start_pos:], start_pos):
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        # Found complete rule
                                        complete_rule = classification_content[start_pos:i+1]
                                        rule_matches.append(complete_rule)
                                        break
                    else:
                        # Fallback to old pattern
                        rule_pattern = r'rule\s+\w+\s*\{[^}]*\}'
                        rule_matches = re.findall(rule_pattern, classification_content, re.DOTALL | re.IGNORECASE)
                
                if rule_matches:
                    for rule in rule_matches:
                        # Clean up the rule syntax and rename based on input file
                        cleaned_rule = self._clean_yara_rule_syntax(rule, input_file.stem)
                        
                        # Create a unique identifier for this rule to prevent duplicates
                        rule_id = self._create_rule_identifier(cleaned_rule)
                        
                        if rule_id not in processed_rules:
                            processed_rules.add(rule_id)
                            yara_rules.append(cleaned_rule)
                            logger.info(f"üìã Extracted YARA rule from {input_file.name}")
                        else:
                            logger.info(f"üîÑ Skipping duplicate rule from {input_file.name}")
                else:
                    logger.warning(f"‚ö†Ô∏è  No YARA rule found in classification results for {input_file.name}")
                    
            except Exception as e:
                logger.error(f"‚ùå Failed to read classification results for {input_file.name}: {e}")
                continue
        
        return yara_rules
    
    def _clean_yara_rule_syntax(self, rule: str, input_stem: str) -> str:
        """Clean up YARA rule syntax to ensure proper formatting"""
        import re
        
        # Remove any markdown formatting
        rule = rule.replace('```yara', '').replace('```', '')
        
        # Ensure proper spacing
        rule = re.sub(r'\n\s*\n', '\n', rule)
        rule = rule.strip()
        
        # Rename rule to match input file stem
        rule_name_match = re.search(r'rule\s+(\w+)', rule)
        if rule_name_match:
            old_rule_name = rule_name_match.group(1)
            
            # Ensure the new rule name doesn't start with a number
            # YARA rule names must start with a letter or underscore
            # Replace dashes and spaces with underscores
            clean_stem = input_stem.replace('-', '_').replace(' ', '_') if input_stem else ""
            
            if clean_stem and clean_stem[0].isdigit():
                new_rule_name = f"file_{clean_stem}"
            else:
                new_rule_name = clean_stem
            
            # Replace the old rule name with the new rule name
            rule = re.sub(rf'rule\s+{re.escape(old_rule_name)}\s*{{', f'rule {new_rule_name} {{', rule)
        
        return rule
    
    def _create_rule_identifier(self, rule: str) -> str:
        """Create a unique identifier for a YARA rule to help with deduplication"""
        import re
        
        # Extract rule name
        rule_name_match = re.search(r'rule\s+(\w+)', rule)
        rule_name = rule_name_match.group(1) if rule_name_match else "unknown"
        
        # Extract key strings (first few)
        strings_matches = re.findall(r'\$\w+\s*=\s*"([^"]+)"', rule)
        key_strings = strings_matches[:3]  # Use first 3 strings
        
        # Create identifier from rule name and key strings
        identifier = f"{rule_name}:{':'.join(key_strings)}"
        return identifier.lower()
    
    def _create_empty_json_structure(self) -> Path:
        """Create an empty JSON structure for when no text files are present"""
        empty_json = {
            'cleanup_constants': [],
            'signatures': []
        }
        
        output_file = self.data_dir / "signatures_empty.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(empty_json, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÅ Created empty JSON structure: {output_file}")
        return output_file
    
    def _append_binary_yara_rules(self):
        """Append binary file YARA rules to the existing YARA file"""
        try:
            # Read existing YARA file
            with open(self.yara_file, 'r', encoding='utf-8') as f:
                existing_content = f.read()
            
            # Add binary file rules
            binary_rules_content = "\n\n" + "\n\n".join(self.binary_yara_rules)
            
            # Write back to file
            with open(self.yara_file, 'w', encoding='utf-8') as f:
                f.write(existing_content + binary_rules_content)
            
            logger.info(f"‚úÖ Appended {len(self.binary_yara_rules)} binary file rules to {self.yara_file}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to append binary file rules: {e}")
    
    def run_transpile_to_yara(self):
        """Convert JSON signatures to YARA rules"""
        logger.info("üîÑ Step 2: Converting JSON to YARA rules...")
        
        try:
            # Run transpile_to_yara.py
            cmd = [
                sys.executable,
                str(self.transpile_script),
                str(self.json_file),
                str(self.yara_file)
            ]
            
            logger.debug(f"üîß Running command: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=self.workspace_root,
                check=True,
                timeout=60
            )
            
            if result.returncode == 0:
                logger.info(f"‚úÖ Successfully converted JSON to YARA rules: {self.yara_file}")
                
                # Append binary file rules if any
                if hasattr(self, 'binary_yara_rules') and self.binary_yara_rules:
                    self._append_binary_yara_rules()
                
                logger.info("‚úÖ Step 2 completed")
            else:
                logger.error(f"‚ùå transpile_to_yara.py failed with exit code {result.returncode}")
                raise subprocess.CalledProcessError(result.returncode, cmd)
                
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå transpile_to_yara.py failed: {e}")
            raise
        except Exception as e:
            logger.error(f"‚ùå Unexpected error in transpile_to_yara: {e}")
            raise
    
    def run_llm_validation(self, output_file: str = None):
        """Run LLM validation on the generated YARA rules"""
        logger.info("üîÑ Step 3: Running LLM validation...")
        
        try:
            # Check if validation script exists
            if not self.validation_script.exists():
                logger.warning("‚ö†Ô∏è  LLM validation script not found, skipping validation")
                return
            
            # Build validation command
            cmd = [
                sys.executable,
                str(self.validation_script),
                str(self.yara_file),
                "--json-file", str(self.json_file)
            ]
            
            if output_file:
                cmd.extend(["--output", output_file])
            
            logger.debug(f"üîß Running command: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=self.workspace_root,
                check=True,
                timeout=300  # 5 minutes for validation
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ LLM validation completed successfully")
                logger.info("‚úÖ Step 3 completed")
            else:
                logger.error(f"‚ùå LLM validation failed with exit code {result.returncode}")
                
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå LLM validation failed: {e}")
        except Exception as e:
            logger.error(f"‚ùå Unexpected error in LLM validation: {e}")
    
    def validate_outputs(self):
        """Validate that the pipeline produced the expected outputs"""
        logger.info("üîç Validating pipeline outputs...")
        
        # Check JSON file
        if self.json_file.exists():
            try:
                with open(self.json_file, 'r') as f:
                    json_data = json.load(f)
                signature_count = len(json_data.get('signatures', []))
                logger.info(f"‚úÖ JSON file created: {self.json_file} ({signature_count} signatures)")
            except Exception as e:
                logger.error(f"‚ùå JSON file validation failed: {e}")
        else:
            logger.error(f"‚ùå JSON file not found: {self.json_file}")
        
        # Check YARA file
        if self.yara_file.exists():
            try:
                with open(self.yara_file, 'r') as f:
                    yara_content = f.read()
                
                # Count YARA rules
                rule_count = len(re.findall(r'^rule\s+\w+', yara_content, re.MULTILINE))
                logger.info(f"‚úÖ YARA file created: {self.yara_file} ({rule_count} rules)")
            except Exception as e:
                logger.error(f"‚ùå YARA file validation failed: {e}")
        else:
            logger.error(f"‚ùå YARA file not found: {self.yara_file}")
    
    def run_pipeline(self, skip_validation: bool = False, output_file: str = None):
        """Run the complete pipeline"""
        try:
            logger.info("üöÄ Starting YARA Pipeline...")
            
            # Check prerequisites
            self.check_prerequisites()
            
            # Step 1: Convert text files to JSON, analyze binary files
            self.run_txt_to_json()
            
            # Step 2: Convert JSON to YARA
            self.run_transpile_to_yara()
            
            # Step 3: LLM validation (optional)
            if not skip_validation:
                self.run_llm_validation(output_file)
            
            # Validate outputs
            self.validate_outputs()
            
            logger.info("üéâ Pipeline completed successfully!")
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            raise

def main():
    parser = argparse.ArgumentParser(description="Run the complete YARA pipeline")
    parser.add_argument("--input", "-i", nargs="+", help="Input files to process")
    parser.add_argument("--data-dir", "-d", default="data", help="Data directory")
    parser.add_argument("--scripts-dir", "-s", default="scripts", help="Scripts directory")
    parser.add_argument("--skip-validation", action="store_true", help="Skip LLM validation")
    parser.add_argument("--output", "-o", help="Output file for validation results")
    
    args = parser.parse_args()
    
    try:
        # Initialize pipeline runner
        runner = PipelineRunner(
            data_dir=args.data_dir,
            scripts_dir=args.scripts_dir,
            input_files=args.input
        )
        
        # Run pipeline
        runner.run_pipeline(
            skip_validation=args.skip_validation,
            output_file=args.output
        )
        
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
